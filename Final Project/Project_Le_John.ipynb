{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28616145-9dba-4d94-95c9-676b54260336",
   "metadata": {},
   "source": [
    "# CSCI 3202: Introduction to Artificial Intelligence, Fall 2023\n",
    "## Final Mancala Project\n",
    "### Your name: John Le\n",
    "â€‹\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb3c882-fb51-41f6-9f85-3d727d00f484",
   "metadata": {},
   "source": [
    "#### Code from Hw05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21b308",
   "metadata": {},
   "source": [
    "## Interface to Play Mancala and Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bae68d0-13ea-4012-b4f4-9921e46976e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed21a264-5505-4fc8-b2a4-470634d24273",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mancala:\n",
    "    def __init__(self, pits_per_player=6, stones_per_pit = 4):\n",
    "        \"\"\"\n",
    "        The constructor for the Mancala class defines several instance variables:\n",
    "\n",
    "        pits_per_player: This variable stores the number of pits each player has.\n",
    "        stones_per_pit: It represents the number of stones each pit contains at the start of any game.\n",
    "        board: This data structure is responsible for managing the Mancala board.\n",
    "        current_player: This variable takes the value 1 or 2, as it's a two-player game, indicating which player's turn it is.\n",
    "        moves: This is a list used to store the moves made by each player. It's structured in the format (current_player, chosen_pit).\n",
    "        p1_pits_index: A list containing two elements representing the start and end indices of player 1's pits in the board data structure.\n",
    "        p2_pits_index: Similar to p1_pits_index, it contains the start and end indices for player 2's pits on the board.\n",
    "        p1_mancala_index and p2_mancala_index: These variables hold the indices of the Mancala pits on the board for players 1 and 2, respectively.\n",
    "        \"\"\"\n",
    "        self.pits_per_player = pits_per_player\n",
    "        self.stones_per_pit = stones_per_pit\n",
    "        self.board = [stones_per_pit] * ((pits_per_player+1) * 2)  # Initialize each pit with stones_per_pit number of stones \n",
    "        self.players = 2\n",
    "        self.current_player = 1\n",
    "        self.moves = []\n",
    "        self.p1_pits_index = [0, self.pits_per_player-1]\n",
    "        self.p1_mancala_index = self.pits_per_player\n",
    "        self.p2_pits_index = [self.pits_per_player+1, len(self.board)-1-1]\n",
    "        self.p2_mancala_index = len(self.board)-1\n",
    "        \n",
    "        # Zeroing the Mancala for both players\n",
    "        self.board[self.p1_mancala_index] = 0\n",
    "        self.board[self.p2_mancala_index] = 0\n",
    "\n",
    "    def display_board(self):\n",
    "        \"\"\"\n",
    "        Displays the board in a user-friendly format\n",
    "        \"\"\"\n",
    "        player_1_pits = self.board[self.p1_pits_index[0]: self.p1_pits_index[1]+1]\n",
    "        player_1_mancala = self.board[self.p1_mancala_index]\n",
    "        player_2_pits = self.board[self.p2_pits_index[0]: self.p2_pits_index[1]+1]\n",
    "        player_2_mancala = self.board[self.p2_mancala_index]\n",
    "\n",
    "        print('P1               P2')\n",
    "        print('     ____{}____     '.format(player_2_mancala))\n",
    "        for i in range(self.pits_per_player):\n",
    "            if i == self.pits_per_player - 1:\n",
    "                print('{} -> |_{}_|_{}_| <- {}'.format(i+1, player_1_pits[i], \n",
    "                        player_2_pits[-(i+1)], self.pits_per_player - i))\n",
    "            else:    \n",
    "                print('{} -> | {} | {} | <- {}'.format(i+1, player_1_pits[i], \n",
    "                        player_2_pits[-(i+1)], self.pits_per_player - i))\n",
    "            \n",
    "        print('         {}         '.format(player_1_mancala))\n",
    "        turn = 'P1' if self.current_player == 1 else 'P2'\n",
    "        print('Turn: ' + turn)\n",
    "        \n",
    "    def valid_move(self, pit):\n",
    "        \"\"\"\n",
    "        Function to check if the pit chosen by the current_player is a valid move.\n",
    "        \"\"\"\n",
    "        \n",
    "        # write your code here\n",
    "        start_index, end_index = (\n",
    "            self.p1_pits_index if self.current_player == 1 else self.p2_pits_index\n",
    "        )\n",
    "        if pit < 1 or pit > self.pits_per_player or self.board[start_index + pit - 1] == 0:\n",
    "            #print(\"INVALID MOVE\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "        pass\n",
    "        \n",
    "    def random_move_generator(self):\n",
    "        \"\"\"\n",
    "        Function to generate random valid moves with non-empty pits for the random player\n",
    "        \"\"\"\n",
    "        \n",
    "        pit = random.randint(1, self.pits_per_player)\n",
    "        \n",
    "        while not self.valid_move(pit):\n",
    "            pit = random.randint(1, self.pits_per_player)\n",
    "            \n",
    "        return pit\n",
    "    \n",
    "    def play(self, pit):\n",
    "        \"\"\"\n",
    "        This function simulates a single move made by a specific player using their selected pit. It primarily performs three tasks:\n",
    "        1. It checks if the chosen pit is a valid move for the current player. If not, it prints \"INVALID MOVE\" and takes no action.\n",
    "        2. It verifies if the game board has already reached a winning state. If so, it prints \"GAME OVER\" and takes no further action.\n",
    "        3. After passing the above two checks, it proceeds to distribute the stones according to the specified Mancala rules.\n",
    "\n",
    "        Finally, the function then switches the current player, allowing the other player to take their turn.\n",
    "        \"\"\"\n",
    "        \n",
    "        # write your code here\n",
    "        if not self.valid_move(pit) or self.winning_eval():\n",
    "            #print(f\"Player {self.current_player} chose pit: {pit}\")\n",
    "            return self.board\n",
    "\n",
    "        start_index, end_index = (\n",
    "            self.p1_pits_index if self.current_player == 1 else self.p2_pits_index\n",
    "        )\n",
    "        stones = self.board[start_index + pit - 1]\n",
    "        self.board[start_index + pit - 1] = 0\n",
    "\n",
    "        current_index = start_index + pit\n",
    "        while stones > 0:\n",
    "            if current_index == self.p1_mancala_index and self.current_player == 2:\n",
    "                current_index += 1\n",
    "            elif current_index == self.p2_mancala_index and self.current_player == 1:\n",
    "                current_index = start_index\n",
    "            else:\n",
    "                self.board[current_index] += 1\n",
    "                stones -= 1\n",
    "                current_index = (current_index + 1) % len(self.board)\n",
    "\n",
    "        # Check for capturing on the current player's side\n",
    "        last_index = (current_index - 1) % len(self.board)\n",
    "        opposite_pit_index = len(self.board) - 2 - last_index\n",
    "        current_player_pits_start, current_player_pits_end = (\n",
    "            self.p1_pits_index if self.current_player == 1 else self.p2_pits_index\n",
    "        )\n",
    "        if (\n",
    "            last_index != self.p1_mancala_index\n",
    "            and last_index != self.p2_mancala_index\n",
    "            and current_player_pits_start <= last_index <= current_player_pits_end\n",
    "            and self.board[last_index] == 1\n",
    "            and self.board[opposite_pit_index] > 0\n",
    "        ):\n",
    "            self.board[self.p1_mancala_index if self.current_player == 1 else self.p2_mancala_index] += (\n",
    "                self.board[last_index] + self.board[opposite_pit_index]\n",
    "            )\n",
    "            self.board[last_index] = 0\n",
    "            self.board[opposite_pit_index] = 0\n",
    "\n",
    "        # Switch player after move\n",
    "        self.current_player = 3 - self.current_player  # Switch between player 1 and player 2\n",
    "\n",
    "        # Save the move\n",
    "        self.moves.append((3 - self.current_player, pit))\n",
    "        \n",
    "        #print(f\"Player {3 - self.current_player} chose pit: {pit}\")\n",
    "\n",
    "        return self.board\n",
    "    \n",
    "    def evaluate_state(self):\n",
    "        return self.board[self.p1_mancala_index] - self.board[self.p2_mancala_index]\n",
    "    \n",
    "    def winning_eval(self):\n",
    "        \"\"\"\n",
    "        Function to verify if the game board has reached the winning state.\n",
    "        Hint: If either of the players' pits are all empty, then it is considered a winning state.\n",
    "        \"\"\"\n",
    "        \n",
    "        # write your code here\n",
    "        if all(self.board[i] == 0 for i in range(self.p1_pits_index[0], self.p1_pits_index[1] + 1)):\n",
    "            self.board[self.p2_mancala_index] += sum(self.board[self.p2_pits_index[0] : self.p2_pits_index[1] + 1])\n",
    "            return True\n",
    "        elif all(self.board[i] == 0 for i in range(self.p2_pits_index[0], self.p2_pits_index[1] + 1)):\n",
    "            self.board[self.p1_mancala_index] += sum(self.board[self.p1_pits_index[0] : self.p1_pits_index[1] + 1])\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "        pass\n",
    "    \n",
    "    def generate_board(self):\n",
    "        self.board = [self.stones_per_pit] * ((self.pits_per_player + 1) * 2)\n",
    "        self.current_player = 1\n",
    "        self.moves = []\n",
    "        self.board[self.p1_mancala_index] = 0\n",
    "        self.board[self.p2_mancala_index] = 0\n",
    "    \n",
    "    def match_analysis(self, games=100, ai_player=None, alpha_beta=False):\n",
    "        self.p1_wins = 0\n",
    "        self.p2_wins = 0\n",
    "        self.ties = 0\n",
    "        total_moves = 0\n",
    "\n",
    "        for _ in range(games):\n",
    "            self.generate_board()\n",
    "            moves_in_current_game = 0\n",
    "\n",
    "            while not self.winning_eval():\n",
    "                if ai_player and self.current_player == 2:\n",
    "                    move = ai_player.best_move(self, alpha_beta)\n",
    "                else:\n",
    "                    move = self.random_move_generator()\n",
    "                self.play(move)\n",
    "                moves_in_current_game += 1\n",
    "\n",
    "            total_moves += moves_in_current_game\n",
    "\n",
    "            if self.board[self.p1_mancala_index] > self.board[self.p2_mancala_index]:\n",
    "                self.p1_wins += 1\n",
    "            elif self.board[self.p1_mancala_index] < self.board[self.p2_mancala_index]:\n",
    "                self.p2_wins += 1\n",
    "            else:\n",
    "                self.ties += 1\n",
    "\n",
    "        average_moves = total_moves / games\n",
    "        print(f\"P1 Wins: {self.p1_wins}, P2 Wins: {self.p2_wins}, Ties: {self.ties}\")\n",
    "        print(f\"Average moves to win: {average_moves}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e49475",
   "metadata": {},
   "source": [
    "### Displays Board with Pre-moves determined (uncomment to see)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84124b75-7c78-452a-9ef1-3361705bb895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Mancala part 1 \n",
    "\n",
    "# game = Mancala()\n",
    "# game.display_board()\n",
    "\n",
    "# # Player 1 selects pit 1 (1-based index)\n",
    "# game.play(1)\n",
    "# game.display_board()\n",
    "\n",
    "# # Player 2 selects pit 2\n",
    "# game.play(2)\n",
    "# game.display_board()\n",
    "\n",
    "# # Player 1 selects pit 3\n",
    "# game.play(3)\n",
    "# game.display_board()\n",
    "\n",
    "# # Player 2 selects pit 2\n",
    "# game.play(2)\n",
    "# game.display_board()\n",
    "\n",
    "# # Player 1 selects pit 1\n",
    "# game.play(1)\n",
    "# game.display_board()\n",
    "\n",
    "# # Printing the list of moves\n",
    "# print(\"\\nList of valid moves:\")\n",
    "# for move in game.moves:\n",
    "#     player, pit = move\n",
    "#     print(f\"Player {player} selected pit {pit}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87676f69",
   "metadata": {},
   "source": [
    "### Generate Game against a Random Player (uncomment to play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca51f40-9f85-4a92-a66e-edc6af0336bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mancala part 2 \n",
    "\n",
    "# game = Mancala(pits_per_player=6, stones_per_pit=4)\n",
    "# game.display_board()\n",
    "\n",
    "# # Play up to 5 moves by the player and 5 moves by the random player\n",
    "# total_moves = 10\n",
    "# for _ in range(total_moves):\n",
    "#     if game.current_player == 1:\n",
    "#         # Player's move\n",
    "#         player_move = int(input())\n",
    "#         game.play(player_move)\n",
    "#     else:\n",
    "#         # Random player's move\n",
    "#         random_move = game.random_move_generator()\n",
    "#         game.play(random_move)\n",
    "\n",
    "#     game.display_board()\n",
    "\n",
    "#     if game.winning_eval():\n",
    "#         print(\"GAME OVER\")\n",
    "#         break \n",
    "\n",
    "# # Printing the list of moves\n",
    "# print(\"\\nList of valid moves:\")\n",
    "# for move in game.moves:\n",
    "#     player, pit = move\n",
    "#     print(f\"Player {player} selected pit {pit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc00965-3722-404d-bd4d-9a224d7349e0",
   "metadata": {},
   "source": [
    "## Implementing AI to Play Mancala (new code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f3e083b-a217-48fa-8c75-9df2d0b002e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math, time, random\n",
    "random.seed(109)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d44a8c7",
   "metadata": {},
   "source": [
    "#### Random player vs. Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb513355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Wins: 44, P2 Wins: 48, Ties: 8\n",
      "Average moves to win: 43.76\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the Mancala class\n",
    "game = Mancala()\n",
    "\n",
    "# Play 100 games of random player against random player\n",
    "game.match_analysis(games=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc85310e-ca94-4ca0-b8c1-8c2b5d0cf73a",
   "metadata": {},
   "source": [
    "3. Play 100 games of random player against random player\n",
    "    - What percentage of games does each player (1st or 2nd) win?\n",
    "        - `On average, for a random vs random simulation, the percentage that each player wins ranges from 47-50% and around a 2-3% tie rate. The wins for each player are close to being a 50-50 split which is what I believe should be happening.`\n",
    "    - On average, how many moves does it take to win?\n",
    "        - `On average, it ranges around 42-46 moves to win a game.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e350f1-828c-405b-ac35-2d3c1ab55208",
   "metadata": {},
   "source": [
    "# Intermediate Results\n",
    "\n",
    "As of today, I thought that I had completed the initial phase of the project, but it turned out I had an error in my **'random_move_generator'** that made the random player stuck on one invalid move infinitely after a certain amount of moves. I managed to figure it out and now I am working on the next step of the project which is to immplement the minimax and Alpha-Beta players. Everything so far seems to work on my project, I just haven't tested the minimax player or implemented the other yet. And since hw05, I have just been trying to figure out why the error I previously mentioned was happening and I figured it out a bit too late for this Intermediate Results deadline. I haven't done that much outside of hw05 but I will get a lot more done in the coming week before the deadline of the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242aad9",
   "metadata": {},
   "source": [
    "## Mancala Minimax AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "644d1663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MancalaAI:\n",
    "    def __init__(self, depth, state):\n",
    "        # Constructor to initialize the AI with a given search depth and game state\n",
    "        self.depth = depth\n",
    "        self.state = state\n",
    "    \n",
    "    # Define a minimax function for game-playing AI\n",
    "    def minimax(self, state, depth, maximizing_player, cur_player):\n",
    "        # Base case: if the depth is 0 or the current state is a winning state, return the evaluation of the state\n",
    "        if depth == 0 or state.winning_eval():\n",
    "            return self.evaluate_state(state)\n",
    "\n",
    "        # Get a list of possible valid moves for the current state\n",
    "        possible_valid_moves = self.get_valid_moves(state)\n",
    "        \n",
    "        # If it's the turn of the maximizing player\n",
    "        if maximizing_player:\n",
    "            # Initialize max_eval to negative infinity\n",
    "            max_eval = float('-inf')\n",
    "            # Iterate through each possible move\n",
    "            for move in possible_valid_moves:\n",
    "                # Simulate the move and get the new state\n",
    "                new_state = self.simulate_move(state, move)\n",
    "                # Recursively call minimax with the new state, decreasing depth, and switching players\n",
    "                eval = self.minimax(new_state, depth - 1, False, 3 - cur_player)\n",
    "                # Update max_eval with the maximum value between max_eval and eval\n",
    "                max_eval = max(max_eval, eval)\n",
    "            # Return the maximum evaluation for the maximizing player's turn\n",
    "            return max_eval\n",
    "        # If it's the turn of the minimizing player\n",
    "        else:\n",
    "            # Initialize min_eval to positive infinity\n",
    "            min_eval = float('inf')\n",
    "            # Iterate through each possible move\n",
    "            for move in possible_valid_moves:\n",
    "                # Simulate the move and get the new state\n",
    "                new_state = self.simulate_move(state, move)\n",
    "                # Recursively call minimax with the new state, decreasing depth, and switching players\n",
    "                eval = self.minimax(new_state, depth - 1, True, 3 - cur_player)\n",
    "                # Update min_eval with the minimum value between min_eval and eval\n",
    "                min_eval = min(min_eval, eval)\n",
    "            # Return the minimum evaluation for the minimizing player's turn\n",
    "            return min_eval\n",
    "    \n",
    "    # Define a minimax algorithm with alpha-beta pruning for game-playing AI\n",
    "    def minimax_alpha_beta(self, state, depth, alpha, beta, maximizing_player, cur_player):\n",
    "        # Base case: if the depth is 0 or the current state is a winning state, return the evaluation of the state\n",
    "        if depth == 0 or state.winning_eval():\n",
    "            return self.evaluate_state(state)\n",
    "\n",
    "        # Get a list of possible valid moves for the current state\n",
    "        possible_valid_moves = self.get_valid_moves(state)\n",
    "\n",
    "        # If it's the turn of the maximizing player\n",
    "        if maximizing_player:\n",
    "            # Initialize max_eval to negative infinity\n",
    "            max_eval = float('-inf')\n",
    "            # Iterate through each possible move\n",
    "            for move in possible_valid_moves:\n",
    "                # Simulate the move and get the new state\n",
    "                new_state = self.simulate_move(state, move)\n",
    "                # Recursively call minimax_alpha_beta with the new state, decreasing depth, and switching players\n",
    "                eval = self.minimax_alpha_beta(new_state, depth - 1, alpha, beta, False, 3 - cur_player)\n",
    "                # Update max_eval with the maximum value between max_eval and eval\n",
    "                max_eval = max(max_eval, eval)\n",
    "                # Update alpha with the maximum value between alpha and eval\n",
    "                alpha = max(alpha, eval)\n",
    "                # If beta is less than or equal to alpha, prune the search and break out of the loop\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            # Return the maximum evaluation for the maximizing player's turn\n",
    "            return max_eval\n",
    "        # If it's the turn of the minimizing player\n",
    "        else:\n",
    "            # Initialize min_eval to positive infinity\n",
    "            min_eval = float('inf')\n",
    "            # Iterate through each possible move\n",
    "            for move in possible_valid_moves:\n",
    "                # Simulate the move and get the new state\n",
    "                new_state = self.simulate_move(state, move)\n",
    "                # Recursively call minimax_alpha_beta with the new state, decreasing depth, and switching players\n",
    "                eval = self.minimax_alpha_beta(new_state, depth - 1, alpha, beta, True, 3 - cur_player)\n",
    "                # Update min_eval with the minimum value between min_eval and eval\n",
    "                min_eval = min(min_eval, eval)\n",
    "                # Update beta with the minimum value between beta and eval\n",
    "                beta = min(beta, eval)\n",
    "                # If beta is less than or equal to alpha, prune the search and break out of the loop\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            # Return the minimum evaluation for the minimizing player's turn\n",
    "            return min_eval\n",
    "\n",
    "    # Define a function to find the best move for the current player\n",
    "    def best_move(self, state, alpha_beta=False):\n",
    "        # Get a list of possible valid moves for the current state\n",
    "        possible_valid_moves = self.get_valid_moves(state)\n",
    "        # Initialize variables to store the best move and its corresponding value\n",
    "        best_move = None\n",
    "        best_value = float('-inf') if state.current_player == 1 else float('inf')\n",
    "\n",
    "        # Iterate through each possible move\n",
    "        for move in possible_valid_moves:\n",
    "            # Simulate the move and get the new state\n",
    "            new_state = self.simulate_move(state, move)\n",
    "            # Use either minimax with alpha-beta pruning or basic minimax to get the value of the move\n",
    "            value = self.minimax_alpha_beta(new_state, self.depth, float('-inf'), float('inf'), not state.current_player == 1, state.current_player) if alpha_beta else self.minimax(new_state, self.depth, not state.current_player == 1, state.current_player)\n",
    "\n",
    "            # Compare the obtained value with the best value based on the current player\n",
    "            if (state.current_player == 1 and value > best_value) or (state.current_player == 2 and value < best_value):\n",
    "                # If the obtained value is better, update the best value and move\n",
    "                best_value = value\n",
    "                best_move = move\n",
    "\n",
    "        # Return the best move found\n",
    "        return best_move\n",
    "\n",
    "    # Define a function to evaluate the given game state\n",
    "    def evaluate_state(self, state):\n",
    "        # Basic scoring based on Mancala count\n",
    "        score = state.board[state.p1_mancala_index] - state.board[state.p2_mancala_index]\n",
    "\n",
    "        # Factor in potential captures and board control\n",
    "        for i in range(state.pits_per_player):\n",
    "            # Evaluate player 1's side\n",
    "            if state.board[i] == (state.pits_per_player - i):\n",
    "                score += 2  # Potential capture or consecutive turn\n",
    "            if state.board[i] > 0:\n",
    "                score += 1  # Board control\n",
    "\n",
    "            # Evaluate player 2's side\n",
    "            opp_index = state.p2_pits_index[0] + i\n",
    "            if state.board[opp_index] == (state.pits_per_player - i):\n",
    "                score -= 2\n",
    "            if state.board[opp_index] > 0:\n",
    "                score -= 1\n",
    "\n",
    "        return score\n",
    "\n",
    "    # Define a function to get valid moves for the current state\n",
    "    def get_valid_moves(self, state):\n",
    "        # Helper function to get valid moves for the current state\n",
    "        valid_moves = []\n",
    "        # Iterate through each pit for the current player\n",
    "        for i in range(1, state.pits_per_player + 1):\n",
    "            # Check if the move is valid for the current pit\n",
    "            if state.valid_move(i):\n",
    "                # Add the valid move to the list\n",
    "                valid_moves.append(i)\n",
    "        # Return the list of valid moves\n",
    "        return valid_moves\n",
    "\n",
    "    # Define a function to simulate a move without modifying the original state\n",
    "    def simulate_move(self, state, move):\n",
    "        # Helper function to simulate a move without modifying the original state\n",
    "        # Create a deep copy of the current state to avoid modifying the original state\n",
    "        new_state = copy.deepcopy(state)\n",
    "        # Simulate the move on the copied state\n",
    "        new_state.play(move)\n",
    "        # Return the modified state after the simulated move\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f27efd",
   "metadata": {},
   "source": [
    "#### Minimax Player vs. Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b08a118d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Wins: 1, P2 Wins: 99, Ties: 0\n",
      "Average moves to win: 32.92\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the Mancala class\n",
    "game = Mancala()\n",
    "\n",
    "# Create an instance of the Minimax AI player with a depth of 5 plies\n",
    "ai_player = MancalaAI(depth=5, state=game)\n",
    "\n",
    "# Play 100 games with the random player against the Minimax AI player\n",
    "game.match_analysis(games=100, ai_player=ai_player)\n",
    "\n",
    "# Display the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c69729",
   "metadata": {},
   "source": [
    "4. Build an AI player that uses minimax to choose the best move with a variable number of plies and a utility function we describe\n",
    "    - What percentage of games does each player (AI or random) win?\n",
    "        - `The AI player has a win percentage of over 80% against a random player.`\n",
    "    - On average, how many moves does it take to win?\n",
    "        - `On average, it ranges around 31-34 moves to win a game.`\n",
    "5. Play 100 games with the random player against the minimax AI player at a depth of 5 plies\n",
    "    - What percentage of games does each player (AI or random) win?\n",
    "        - `On average, for a minimax vs random simulation, the percentage that the minimax player wins ranges from ~98-100%, around a ~1-2% random player winrate, and a ~1% tie rate. The minimax player should win most of the time and the random player may win a handful of times.`\n",
    "    - On average, how many moves does it take to win?\n",
    "        - `On average, it ranges around 30-32 moves to win a game.`\n",
    "    - Is your AI player better than random chance? Write a paragraph or two describing or why not\n",
    "        - `The performance of the Minimax AI player, with a 99% win rate against the random player, clearly indicates that it is significantly better than random chance. This outcome is not surprising given the nature of the Minimax algorithm, which systematically evaluates possible future moves and their outcomes to make the best decision at each turn.`\n",
    "\n",
    "            `In contrast, a random player lacks strategic decision-making and simply chooses moves without considering their implications, leading to less optimized play. The Minimax algorithm's ability to look ahead and evaluate game states based on a utility function gives it a substantial advantage. This is evident from the high win rate and the relatively consistent number of moves required to win a game.`\n",
    "\n",
    "            `The fact that the AI wins the vast majority of games and does so with a fairly consistent number of moves suggests that the AI is not only exploiting the random player's lack of strategy but is also efficiently navigating towards winning end-game scenarios. This efficiency reflects the effectiveness of the utility function and the depth of the lookahead used in the algorithm. It demonstrates that the Minimax AI is making moves that consistently lead to advantageous positions, ultimately culminating in wins far more often than what would be expected by chance.`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13a64d5",
   "metadata": {},
   "source": [
    "## Alpha-Beta AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bedf0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MancalaAI:\n",
    "    def __init__(self, depth, state):\n",
    "        self.depth = depth\n",
    "        self.state = state\n",
    "\n",
    "    # Define a minimax function for game-playing AI (same as above)\n",
    "    def minimax(self, state, depth, maximizing_player, cur_player):\n",
    "        if depth == 0 or state.winning_eval():\n",
    "            return self.evaluate_state(state)\n",
    "\n",
    "        possible_valid_moves = self.get_valid_moves(state)\n",
    "        \n",
    "        if maximizing_player:\n",
    "            max_eval = float('-inf')\n",
    "            for move in possible_valid_moves:\n",
    "                new_state = self.simulate_move(state, move)\n",
    "                eval = self.minimax(new_state, depth - 1, False, 3 - cur_player)\n",
    "                max_eval = max(max_eval, eval)\n",
    "            return max_eval\n",
    "        else:\n",
    "            min_eval = float('inf')\n",
    "            for move in possible_valid_moves:\n",
    "                new_state = self.simulate_move(state, move)\n",
    "                eval = self.minimax(new_state, depth - 1, True, 3 - cur_player)\n",
    "                min_eval = min(min_eval, eval)\n",
    "            return min_eval\n",
    "\n",
    "    # New Alpha-Beta Pruning method\n",
    "    def alpha_beta_pruning(self, state, depth, alpha, beta, maximizing_player):\n",
    "        if depth == 0 or state.winning_eval():\n",
    "            return self.evaluate_state(state)\n",
    "\n",
    "        if maximizing_player:\n",
    "            max_eval = float('-inf')\n",
    "            for move in self.get_valid_moves(state):\n",
    "                new_state = self.simulate_move(state, move)\n",
    "                eval = self.alpha_beta_pruning(new_state, depth - 1, alpha, beta, False)\n",
    "                max_eval = max(max_eval, eval)\n",
    "                alpha = max(alpha, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            return max_eval\n",
    "        else:\n",
    "            min_eval = float('inf')\n",
    "            for move in self.get_valid_moves(state):\n",
    "                new_state = self.simulate_move(state, move)\n",
    "                eval = self.alpha_beta_pruning(new_state, depth - 1, alpha, beta, True)\n",
    "                min_eval = min(min_eval, eval)\n",
    "                beta = min(beta, eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            return min_eval\n",
    "\n",
    "    # Updated best_move to use Alpha-Beta Pruning\n",
    "    def best_move(self, state, alpha_beta=False):\n",
    "        possible_valid_moves = self.get_valid_moves(state)\n",
    "        best_move = None\n",
    "        best_value = float('-inf') if state.current_player == 1 else float('inf')\n",
    "\n",
    "        for move in possible_valid_moves:\n",
    "            new_state = self.simulate_move(state, move)\n",
    "            value = None\n",
    "            if alpha_beta:\n",
    "                value = self.alpha_beta_pruning(new_state, self.depth, float('-inf'), float('inf'), not state.current_player == 1)\n",
    "            else:\n",
    "                value = self.minimax(new_state, self.depth, not state.current_player == 1, state.current_player)\n",
    "\n",
    "            if (state.current_player == 1 and value > best_value) or (state.current_player == 2 and value < best_value):\n",
    "                best_value = value\n",
    "                best_move = move\n",
    "\n",
    "        return best_move\n",
    "\n",
    "    # Define a function to evaluate the given game state\n",
    "    def evaluate_state(self, state):\n",
    "        # Basic scoring based on Mancala count\n",
    "        score = state.board[state.p1_mancala_index] - state.board[state.p2_mancala_index]\n",
    "\n",
    "        # Factor in potential captures and board control\n",
    "        for i in range(state.pits_per_player):\n",
    "            # Evaluate player 1's side\n",
    "            if state.board[i] == (state.pits_per_player - i):\n",
    "                score += 2  # Potential capture or consecutive turn\n",
    "            if state.board[i] > 0:\n",
    "                score += 1  # Board control\n",
    "\n",
    "            # Evaluate player 2's side\n",
    "            opp_index = state.p2_pits_index[0] + i\n",
    "            if state.board[opp_index] == (state.pits_per_player - i):\n",
    "                score -= 2\n",
    "            if state.board[opp_index] > 0:\n",
    "                score -= 1\n",
    "\n",
    "        return score\n",
    "\n",
    "    # Define a function to get valid moves for the current state (same as above)\n",
    "    def get_valid_moves(self, state):\n",
    "        # Helper function to get valid moves for the current state\n",
    "        valid_moves = []\n",
    "        for i in range(1, state.pits_per_player + 1):\n",
    "            if state.valid_move(i):\n",
    "                valid_moves.append(i)\n",
    "        return valid_moves\n",
    "\n",
    "    # Define a function to simulate a move without modifying the original state (same as above)\n",
    "    def simulate_move(self, state, move):\n",
    "        # Helper function to simulate a move without modifying the original state\n",
    "        new_state = copy.deepcopy(state)\n",
    "        new_state.play(move)\n",
    "        return new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416cacb",
   "metadata": {},
   "source": [
    "#### Alpha-Beta Player vs. Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf2fe3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Wins: 0, P2 Wins: 99, Ties: 1\n",
      "Average moves to win: 31.08\n"
     ]
    }
   ],
   "source": [
    "# Test Alpha-Beta AI against Random Player\n",
    "game = Mancala()\n",
    "alpha_beta_ai = MancalaAI(depth=5, state=game)\n",
    "\n",
    "# Use alpha_beta=True to engage the Alpha-Beta Pruning AI\n",
    "game.match_analysis(games=100, ai_player=alpha_beta_ai, alpha_beta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4070f657",
   "metadata": {},
   "source": [
    "6. Build an AI player that uses Alpha-Beta to choose the best move\n",
    "7. Play 100 games with the random player against the Alpha-Beta AI player at a depth of 5 plies\n",
    "    - How long does it take for a single game to run to completion?\n",
    "        - `On average, it only takes around 1m 40s for all 100 games to be completed. This would mean that it takes around 1s to complete a single game.`\n",
    "    - What percentage of games does each player (AI or random) win?\n",
    "        - `Similar to the Minimax player, the Alpha-Beta Player has a win percentage from between ~98-100% while the Random player will rarely win at a percentage of ~1-2%. The tie rate is also ~1% as well.`\n",
    "    - On average, how many moves does it take to win?\n",
    "        - `On average, it still takes around 30-32 moves to win a game of Mancala.`\n",
    "    - Are your results for this part different from those for your minimax AI player? Write a paragraph or two describing why or why not?\n",
    "        - `The win rates for both the Minimax and Alpha-Beta algorithms are identical, with the AI winning 99% of games in both cases. This similarity is expected as Alpha-Beta pruning and Minimax are fundamentally the same in terms of decision-making and strategy. They both use the same utility function and decision criteria; the only difference lies in their efficiency.`\n",
    "\n",
    "            `However, a significant difference is observed in the runtime. The Alpha-Beta algorithm shows a marked improvement in efficiency, with an average game taking considerably less time (1 minute and 40 seconds) compared to the Minimax algorithm (6 minutes and 45 seconds). This difference is due to the pruning mechanism of the Alpha-Beta algorithm, which effectively skips evaluating parts of the game tree that cannot affect the final decision. This reduction in the number of evaluated nodes results in faster computation times.`\n",
    "\n",
    "            `In summary, while the decision-making quality of the AI remains consistent between Minimax and Alpha-Beta, the latter proves to be more time-efficient due to its ability to eliminate irrelevant branches in the game tree. This demonstrates the practical advantage of Alpha-Beta pruning in scenarios where computation time is a critical factor.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccd814e",
   "metadata": {},
   "source": [
    "#### Alpha-Beta Player vs. Random Player (10plies) [actually 6 because 10 took way to long and it wasn't completed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "100ae204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Wins: 0, P2 Wins: 100, Ties: 0\n",
      "Average moves to win: 31.59\n"
     ]
    }
   ],
   "source": [
    "# Test Alpha-Beta AI against Random Player\n",
    "game = Mancala()\n",
    "alpha_beta_ai = MancalaAI(depth=6, state=game)\n",
    "\n",
    "# Use alpha_beta=True to engage the Alpha-Beta Pruning AI\n",
    "game.match_analysis(games=100, ai_player=alpha_beta_ai, alpha_beta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce039b",
   "metadata": {},
   "source": [
    "8. (Extra Credit, 10 points). Play 100 games with the random player against the Alpha-Beta AI player at a depth of 10 plies\n",
    "    - How long does it take for a single game to run to completion?\n",
    "        - `It took over 100 minutes and the games still wasn't finished so I had to stop it destroying my laptop. Even when I increased the depth of the plies to 6, the runtime increased from 1m 40s to 5m 20s which is 220% increase so increasing it to 10 plies may actually take a full day to complete.`\n",
    "    - What percentage of games does each player (AI or random) win?\n",
    "        - `I would believe that the percentage that the AI would win with a depth of 10 plies would be 100%. The depth of 6 plies already has a 100% win percentage for teh Alpha-beta player so I would assume it would be the same for any depth of plies greater than 5.`\n",
    "    - On average, how many moves does it take to win?\n",
    "        - `The average amount of moves to win the game remains the same though, it takes ~31 moves for the Alpha-Beta player to win a game.`\n",
    "    - Does increasing the number of plies improve the play for our AI player? Why or why not?\n",
    "        - `Even increasing the number of plies by 1 improves the play for out AI but it does increase the completion time of the games. This happends because the AI has Deeper Strategic Planning, a Better Evaluation of Game States, and Improved Handling of Complex Scenarios. The win percentage increases but the runtime also increases so it is like a double-edged sword.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da1c2cb",
   "metadata": {},
   "source": [
    "## Project Write-up and Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eff423d",
   "metadata": {},
   "source": [
    "### Project Overview\n",
    "\n",
    "This project encompasses several stages:\n",
    "\n",
    "`Basic Mancala Interface`: An interface for playing Mancala was developed. This included displaying the game board, prompting for moves, checking for legal moves, and determining game outcomes.\n",
    "\n",
    "`Random Player`: A player making random legal moves was implemented. This served as a baseline opponent for the AI.\n",
    "\n",
    "`Minimax and Alpha-Beta AI`: Two AI players were created using Minimax and Alpha-Beta pruning algorithms. These AIs calculated the best moves based on a utility function that evaluated the game state.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Testing and Evaluation\n",
    "\n",
    "The Random Player and AIs were tested in Various different scenarios:\n",
    "\n",
    "`Random vs Random`: This tested the baseline scenario, with both players making random moves.\n",
    "\n",
    "Results:\n",
    "- The win percentage of the random players would be ~50% win rate for each player\n",
    "- The average amount of moves for them to win was ~42-46 moves\n",
    "- THe runtime of a 100-game simulation was nearly instant\n",
    "\n",
    "`Minimax vs Random`: The Minimax AI was against a Random player to test the efficiency of the algorithms developed for Minimax.\n",
    "\n",
    "Results:\n",
    "- The win percentage for the Minimax AI was significantly greater than the Random player. With the depth of 5 plies, the win percentage of Minimax ranged from ~98-100% while the Random Player rarely wins, and may occasionally have a tie.\n",
    "- The average amount of moves to win a game is also less than the Random vs Random. It takes an average of ~31 moves to win a game with the Minimax algorithm.\n",
    "- The average runtime for the Minimax player vs Random player to complete all 100 games took around 6m 30s. This shows that it takes a while for the Minimax to make decisions to complete the games.\n",
    "\n",
    "`Alpha-Beta vs Random`: The Alpha-Beta AI was against a Random Player to test the efficiency of the alpha-beta pruning algorithm.\n",
    "\n",
    "Results:\n",
    "- The win percentage for the Alpha-Beta player was similar to the Alpha-Beta player which was that the AI, with the depth of 5 plies, had a win percentage that ranged ~98-100% while the Random player would rarely wins, and an occasional tie would happen.\n",
    "- The average amount of moves to win a game is similar to the Minimax AI. It took an average of ~31 moves to win a game with the Alpha-Beta player as well.\n",
    "- Unlike the runtime of the Minimax vs Random simulation, the runtime of the Alpha-Beta was significantly less when running it with the depth of 5 plies. The average runtime of the Alpha-Beta simulation took ~1m 40s which means 1s per game played. This is roughly a 75% improvement between the two different AIs. \n",
    "(When I ran the Alpha-Beta player with the depth of 10plies, the simulation surpassed 100 minutes which I had to stop)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Key Findings and Analysis\n",
    "\n",
    "`Performance of AI Against Random Player`:\n",
    "- The Minimax AI significantly outperformed the random player, winning the majority of the games. This was expected as Minimax evaluates potential future game states, while the random player does not strategize.\n",
    "- The Alpha-Beta AI also demonstrated a high win rate against the random player, similar to the Minimax AI. This similarity is because both algorithms use the same decision-making process, with Alpha-Beta being more efficient in time.\n",
    "\n",
    "`Efficiency of Alpha-Beta Pruning`:\n",
    "- A notable difference was observed in the runtime between the Minimax and Alpha-Beta AIs. The Alpha-Beta AI was faster, thanks to its pruning mechanism that skips irrelevant branches in the game tree. This efficiency makes Alpha-Beta a more practical choice in time-sensitive applications.\n",
    "\n",
    "`AI's Decision-Making Capability`:\n",
    "- Both AIs showed a consistent ability to reach advantageous positions and navigate towards winning end-game scenarios. This consistency indicated effective use of the utility function and the depth of the lookahead in the algorithms.\n",
    "\n",
    "`Utility Function and Game Tree Evaluation`:\n",
    "- The utility function, focusing on the number of stones in each player's Mancala, effectively guided the AI in making strategic decisions. Additionally, the game tree's thorough evaluation by the algorithms ensured that the AI considered all possible outcomes before making a move.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Conclusion\n",
    "The AI application for Mancala successfully demonstrated the power of algorithmic decision-making in board games. The Minimax and Alpha-Beta AIs not only outperformed a random opponent but also did so with a high degree of consistency and efficiency. These findings underline the potential of AI in strategic game-playing scenarios and the effectiveness of algorithms like Minimax and Alpha-Beta pruning in decision-making processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
